/**
 * Classification Processor
 * 
 * Processador de CLASSIFICAÇÃO (metadados COM IA)
 * Responsável por:
 * - Converter documento para Markdown
 * - Extrair dados com IA
 * - Fragmentar documento (chunking)
 * - Gerar embeddings (vetorização)
 * - Indexar no banco
 */

import { db } from '@/lib/db'
import { documents } from '@/lib/db/schema/documents'
import { documentFiles } from '@/lib/db/schema/rag'
import { classificationConfigs } from '@/lib/db/schema/classification-configs'
import { normalizationTemplates } from '@/lib/db/schema/normalization-templates'
import { normalizedData } from '@/lib/db/schema/normalized-data'
import { eq } from 'drizzle-orm'
import { sql } from 'drizzle-orm'
import { convertDocument } from './document-converter'
import { classifyDocument } from './classifier'
import { chunkText } from '@/lib/utils/chunking'
import { generateEmbedding } from './openai'

interface ClassificationResult {
  success: boolean
  documentId: string
  totalChunks?: number
  totalTokens?: number
  error?: string
}

/**
 * Processa a classificação completa de um documento
 * FASE DE IA - extração, chunking, embedding, indexação
 */
export async function processDocumentClassification(
  documentId: string,
  organizationId: string
): Promise<ClassificationResult> {
  try {
    // Buscar documento
    const [document] = await db
      .select()
      .from(documents)
      .where(eq(documents.id, documentId))
      .limit(1)

    if (!document) {
      throw new Error('Documento não encontrado')
    }

    // Verificar se normalização foi concluída
    if (document.normalizationStatus !== 'completed') {
      throw new Error('Documento precisa estar normalizado antes da classificação')
    }

    // Buscar config de classificação
    if (!document.classificationConfigId) {
      throw new Error('Documento não tem configuração de classificação')
    }

    const [config] = await db
      .select()
      .from(classificationConfigs)
      .where(eq(classificationConfigs.id, document.classificationConfigId))
      .limit(1)

    if (!config) {
      throw new Error('Configuração de classificação não encontrada')
    }

    // STEP 1: Extração de Dados com IA
    await db
      .update(documents)
      .set({
        classificationStatus: 'extracting',
        updatedAt: new Date(),
      })
      .where(eq(documents.id, documentId))

    // Converter para Markdown (se ainda não foi convertido)
    const converter = new DocumentConverter()
    let markdown = ''
    try {
      markdown = await converter.convertToMarkdown(document.filePath, document.documentType as any)
    } catch (error) {
      console.error('Erro ao converter documento:', error)
      throw new Error('Erro na conversão do documento')
    }

    // Classificar com IA (extrair dados estruturados)
    let extractedData: any = null

    if (config.enableRAG) {
      try {
        // Buscar template para saber quais campos extrair
        const [template] = await db
          .select()
          .from(normalizationTemplates)
          .where(eq(normalizationTemplates.id, document.normalizationTemplateId!))
          .limit(1)

        if (template) {
          const result = await classifyDocument(
            markdown,
            config.id,
            undefined // schemaConfigId
          )
          extractedData = result.extractedFields

          // Atualizar registro JSONB com dados extraídos
          if (document.customTableRecordId && extractedData) {
            await db
              .update(normalizedData)
              .set({
                data: extractedData, // Salvar dados extraídos no JSONB!
                extractedAt: new Date(),
                extractionConfidence: 0.85, // TODO: pegar do classificador
                updatedAt: new Date(),
              })
              .where(eq(normalizedData.id, document.customTableRecordId))
          }
        }
      } catch (error) {
        console.error('Erro na classificação:', error)
        // Continuar mesmo se a classificação falhar
      }
    }

    // STEP 2: Fragmentação (Chunking)
    await db
      .update(documents)
      .set({
        classificationStatus: 'chunking',
        updatedAt: new Date(),
      })
      .where(eq(documents.id, documentId))

    const chunks = chunkText(markdown, config.chunkSize || 800)

    // STEP 3: Vetorização (Embedding)
    await db
      .update(documents)
      .set({
        classificationStatus: 'embedding',
        updatedAt: new Date(),
      })
      .where(eq(documents.id, documentId))

    // Criar document_file
    const [documentFile] = await db
      .insert(documentFiles)
      .values({
        organizationId,
        userId: document.uploadedBy,
        fileName: document.fileName,
        filePath: document.filePath,
        fileSize: document.fileSize,
        fileHash: document.fileHash,
        mimeType: document.mimeType,
        title: document.title,
        description: document.description,
        totalChunks: chunks.length,
        status: 'completed',
        processedAt: new Date(),
      })
      .returning()

    // Gerar embeddings e salvar chunks
    let totalTokens = 0
    const chunkPromises = chunks.map(async (chunk, index) => {
      const embedding = await generateEmbedding(chunk.content)
      totalTokens += chunk.tokens

      return db.insert(documentChunks).values({
        organizationId,
        documentFileId: documentFile.id,
        content: chunk.content,
        chunkIndex: index,
        tokens: chunk.tokens,
        embedding: embedding,
      })
    })

    await Promise.all(chunkPromises)

    // STEP 4: Finalização
    await db
      .update(documents)
      .set({
        classificationStatus: 'completed',
        classificationCompletedAt: new Date(),
        totalChunks: chunks.length,
        totalTokens,
        totalEmbeddings: chunks.length,
        status: 'completed', // Atualizar status legado também
        processedAt: new Date(),
        updatedAt: new Date(),
      })
      .where(eq(documents.id, documentId))

    // Atualizar estatísticas da config
    await db
      .update(classificationConfigs)
      .set({
        documentsClassified: sql`${classificationConfigs.documentsClassified} + 1`,
        lastUsedAt: new Date(),
        updatedAt: new Date(),
      })
      .where(eq(classificationConfigs.id, config.id))

    return {
      success: true,
      documentId,
      totalChunks: chunks.length,
      totalTokens,
    }
  } catch (error) {
    console.error('Erro na classificação:', error)

    // Atualizar documento com erro
    await db
      .update(documents)
      .set({
        classificationStatus: 'failed',
        classificationError: error instanceof Error ? error.message : 'Erro desconhecido',
        status: 'failed', // Atualizar status legado também
        errorMessage: error instanceof Error ? error.message : 'Erro desconhecido',
        updatedAt: new Date(),
      })
      .where(eq(documents.id, documentId))

    return {
      success: false,
      documentId,
      error: error instanceof Error ? error.message : 'Erro desconhecido',
    }
  }
}

// Removida função updateCustomTableWithExtractedData
// Agora usamos JSONB diretamente - muito mais simples!

/**
 * Verifica se documento está pronto para classificação
 */
export function isReadyForClassification(document: any): boolean {
  return (
    document.normalizationStatus === 'completed' &&
    document.classificationConfigId !== null &&
    document.classificationStatus === 'pending'
  )
}

/**
 * Verifica se classificação foi concluída
 */
export function isClassificationCompleted(document: any): boolean {
  return document.classificationStatus === 'completed'
}

